{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Baselines for Hurst Exponent Estimation\n",
        "\n",
        "**Author:** Davian R. Chin (PhD Candidate in Biomedical Engineering, University of Reading, UK)  \n",
        "**Email:** d.r.chin@pgr.reading.ac.uk  \n",
        "**ORCiD:** [https://orcid.org/0009-0003-9434-3919](https://orcid.org/0009-0003-9434-3919)  \n",
        "**Research Focus:** Physics-Informed Fractional Operator Learning for Real-Time Neurological Biomarker Detection\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This comprehensive tutorial demonstrates how to use the Machine Learning baselines in the Neurological LRD Analysis library for accurate Hurst exponent estimation. We'll cover:\n",
        "\n",
        "1. **Installation and Setup**\n",
        "2. **Feature Extraction** (74+ features from time series)\n",
        "3. **ML Baselines** (Random Forest, SVR, Gradient Boosting)\n",
        "4. **Hyperparameter Optimization** (Optuna Integration)\n",
        "5. **Pretrained Models** (Training, Storage, Loading)\n",
        "6. **Inference Systems** (Single and Batch Predictions)\n",
        "7. **Comprehensive Benchmarking** (Classical vs ML Methods)\n",
        "8. **Performance Analysis** (Accuracy, Speed, Memory)\n",
        "\n",
        "This notebook is designed for researchers and practitioners who want to leverage machine learning for improved Hurst exponent estimation in biomedical time series.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import Neurological LRD Analysis library\n",
        "from neurological_lrd_analysis import (\n",
        "    # Classical methods for comparison\n",
        "    BiomedicalHurstEstimatorFactory,\n",
        "    EstimatorType,\n",
        "    \n",
        "    # ML baselines\n",
        "    MLBaselineType,\n",
        "    RandomForestEstimator,\n",
        "    SVREstimator,\n",
        "    GradientBoostingEstimator,\n",
        "    MLBaselineFactory,\n",
        "    TimeSeriesFeatureExtractor,\n",
        "    \n",
        "    # Hyperparameter optimization\n",
        "    OptunaOptimizer,\n",
        "    create_optuna_study,\n",
        "    optimize_hyperparameters,\n",
        "    optimize_all_estimators,\n",
        "    \n",
        "    # Pretrained models\n",
        "    PretrainedModelManager,\n",
        "    PretrainedInference,\n",
        "    quick_predict,\n",
        "    quick_ensemble_predict,\n",
        "    create_pretrained_suite,\n",
        "    \n",
        "    # Benchmarking\n",
        "    ClassicalMLBenchmark,\n",
        "    run_comprehensive_benchmark\n",
        ")\n",
        "\n",
        "# Import data generation utilities\n",
        "from neurological_lrd_analysis.benchmark_core.generation import (\n",
        "    fbm_davies_harte,\n",
        "    generate_fgn,\n",
        "    generate_arfima,\n",
        "    add_contamination\n",
        ")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(\"🚀 Ready to explore ML baselines for Hurst estimation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Extraction for Time Series\n",
        "\n",
        "The first step in ML-based Hurst estimation is extracting meaningful features from time series data. Our library provides 74+ features across multiple categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample time series data for feature extraction\n",
        "print(\"Generating sample time series data...\")\n",
        "\n",
        "# Create different types of time series with known Hurst exponents\n",
        "hurst_values = [0.3, 0.5, 0.7, 0.9]\n",
        "time_series_data = []\n",
        "true_hurst_values = []\n",
        "\n",
        "for hurst in hurst_values:\n",
        "    # Generate FBM data\n",
        "    fbm_data = fbm_davies_harte(1000, hurst, seed=42)\n",
        "    time_series_data.append(fbm_data)\n",
        "    true_hurst_values.append(hurst)\n",
        "    \n",
        "    # Generate FGN data\n",
        "    fgn_data = generate_fgn(1000, hurst, seed=42)\n",
        "    time_series_data.append(fgn_data)\n",
        "    true_hurst_values.append(hurst)\n",
        "\n",
        "print(f\"Generated {len(time_series_data)} time series\")\n",
        "print(f\"Hurst values range: {min(true_hurst_values):.1f} to {max(true_hurst_values):.1f}\")\n",
        "\n",
        "# Initialize feature extractor\n",
        "feature_extractor = TimeSeriesFeatureExtractor()\n",
        "\n",
        "# Extract features from first time series\n",
        "sample_data = time_series_data[0]\n",
        "print(f\"\\nExtracting features from time series of length {len(sample_data)}...\")\n",
        "\n",
        "# Extract all features\n",
        "features = feature_extractor.extract_features(sample_data)\n",
        "print(f\"✅ Extracted {len(features)} features\")\n",
        "\n",
        "# Display feature categories\n",
        "feature_categories = feature_extractor.get_feature_categories()\n",
        "print(f\"\\nFeature categories available:\")\n",
        "for category, count in feature_categories.items():\n",
        "    print(f\"  - {category}: {count} features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features from all time series for ML training\n",
        "print(\"Extracting features from all time series...\")\n",
        "\n",
        "X_features = []\n",
        "y_targets = []\n",
        "\n",
        "for i, (data, true_hurst) in enumerate(zip(time_series_data, true_hurst_values)):\n",
        "    features = feature_extractor.extract_features(data)\n",
        "    X_features.append(features)\n",
        "    y_targets.append(true_hurst)\n",
        "    \n",
        "    if (i + 1) % 2 == 0:  # Print progress every 2 samples\n",
        "        print(f\"  Processed {i + 1}/{len(time_series_data)} time series\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X_features)\n",
        "y = np.array(y_targets)\n",
        "\n",
        "print(f\"\\n✅ Feature extraction complete!\")\n",
        "print(f\"📊 Feature matrix shape: {X.shape}\")\n",
        "print(f\"🎯 Target vector shape: {y.shape}\")\n",
        "print(f\"📈 Hurst value range: {y.min():.2f} to {y.max():.2f}\")\n",
        "\n",
        "# Display some feature statistics\n",
        "print(f\"\\nFeature statistics:\")\n",
        "print(f\"  - Mean: {X.mean():.4f}\")\n",
        "print(f\"  - Std: {X.std():.4f}\")\n",
        "print(f\"  - Min: {X.min():.4f}\")\n",
        "print(f\"  - Max: {X.max():.4f}\")\n",
        "print(f\"  - NaN values: {np.isnan(X).sum()}\")\n",
        "print(f\"  - Inf values: {np.isinf(X).sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Machine Learning Baselines\n",
        "\n",
        "Now let's explore the three ML baselines: Random Forest, Support Vector Regression (SVR), and Gradient Boosting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Initialize ML estimators\n",
        "print(\"\\nInitializing ML estimators...\")\n",
        "\n",
        "# Random Forest\n",
        "rf_estimator = RandomForestEstimator(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Support Vector Regression\n",
        "svr_estimator = SVREstimator(\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    epsilon=0.1\n",
        ")\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_estimator = GradientBoostingEstimator(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"✅ All ML estimators initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all ML estimators\n",
        "print(\"Training ML estimators...\")\n",
        "\n",
        "estimators = {\n",
        "    'Random Forest': rf_estimator,\n",
        "    'SVR': svr_estimator,\n",
        "    'Gradient Boosting': gb_estimator\n",
        "}\n",
        "\n",
        "training_results = {}\n",
        "\n",
        "for name, estimator in estimators.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Train the estimator\n",
        "    result = estimator.train(X_train, y_train, validation_split=0.2)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Store results\n",
        "    training_results[name] = {\n",
        "        'result': result,\n",
        "        'training_time': training_time,\n",
        "        'estimator': estimator\n",
        "    }\n",
        "    \n",
        "    print(f\"  ✅ Training completed in {training_time:.2f} seconds\")\n",
        "    print(f\"  📊 Training score: {result.training_score:.4f}\")\n",
        "    print(f\"  📊 Validation score: {result.validation_score:.4f}\")\n",
        "\n",
        "print(\"\\n🎉 All estimators trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate ML estimators on test set\n",
        "print(\"Evaluating ML estimators on test set...\")\n",
        "\n",
        "test_results = {}\n",
        "\n",
        "for name, data in training_results.items():\n",
        "    estimator = data['estimator']\n",
        "    \n",
        "    # Make predictions\n",
        "    start_time = time.time()\n",
        "    predictions = estimator.predict(X_test)\n",
        "    prediction_time = time.time() - start_time\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = np.mean((predictions - y_test) ** 2)\n",
        "    mae = np.mean(np.abs(predictions - y_test))\n",
        "    r2 = 1 - (np.sum((y_test - predictions) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
        "    \n",
        "    test_results[name] = {\n",
        "        'predictions': predictions,\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'r2': r2,\n",
        "        'prediction_time': prediction_time\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{name} Performance:\")\n",
        "    print(f\"  📊 MSE: {mse:.4f}\")\n",
        "    print(f\"  📊 MAE: {mae:.4f}\")\n",
        "    print(f\"  📊 R²: {r2:.4f}\")\n",
        "    print(f\"  ⏱️  Prediction time: {prediction_time:.4f} seconds\")\n",
        "\n",
        "print(\"\\n✅ All estimators evaluated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize ML estimator performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: True vs Predicted values\n",
        "ax1 = axes[0, 0]\n",
        "for name, data in test_results.items():\n",
        "    ax1.scatter(y_test, data['predictions'], alpha=0.7, label=name, s=50)\n",
        "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', alpha=0.5)\n",
        "ax1.set_xlabel('True Hurst Exponent')\n",
        "ax1.set_ylabel('Predicted Hurst Exponent')\n",
        "ax1.set_title('True vs Predicted Hurst Exponents')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residuals\n",
        "ax2 = axes[0, 1]\n",
        "for name, data in test_results.items():\n",
        "    residuals = y_test - data['predictions']\n",
        "    ax2.scatter(data['predictions'], residuals, alpha=0.7, label=name, s=50)\n",
        "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
        "ax2.set_xlabel('Predicted Hurst Exponent')\n",
        "ax2.set_ylabel('Residuals')\n",
        "ax2.set_title('Residuals vs Predicted Values')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Performance metrics comparison\n",
        "ax3 = axes[1, 0]\n",
        "metrics = ['MSE', 'MAE', 'R²']\n",
        "x_pos = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "for i, (name, data) in enumerate(test_results.items()):\n",
        "    values = [data['mse'], data['mae'], data['r2']]\n",
        "    ax3.bar(x_pos + i*width, values, width, label=name, alpha=0.8)\n",
        "\n",
        "ax3.set_xlabel('Metrics')\n",
        "ax3.set_ylabel('Values')\n",
        "ax3.set_title('Performance Metrics Comparison')\n",
        "ax3.set_xticks(x_pos + width)\n",
        "ax3.set_xticklabels(metrics)\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Prediction time comparison\n",
        "ax4 = axes[1, 1]\n",
        "names = list(test_results.keys())\n",
        "times = [data['prediction_time'] for data in test_results.values()]\n",
        "bars = ax4.bar(names, times, alpha=0.8, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "ax4.set_ylabel('Prediction Time (seconds)')\n",
        "ax4.set_title('Prediction Speed Comparison')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, time in zip(bars, times):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001, \n",
        "             f'{time:.4f}s', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary table\n",
        "print(\"\\n📊 Performance Summary:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Estimator':<20} {'MSE':<10} {'MAE':<10} {'R²':<10} {'Time (s)':<10}\")\n",
        "print(\"-\" * 80)\n",
        "for name, data in test_results.items():\n",
        "    print(f\"{name:<20} {data['mse']:<10.4f} {data['mae']:<10.4f} {data['r2']:<10.4f} {data['prediction_time']:<10.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Optimization with Optuna\n",
        "\n",
        "Let's optimize the hyperparameters of our ML models using Optuna for better performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimize hyperparameters for all estimators\n",
        "print(\"Optimizing hyperparameters with Optuna...\")\n",
        "\n",
        "# Optimize all estimators\n",
        "optimization_results = optimize_all_estimators(\n",
        "    X_train, y_train,\n",
        "    n_trials=20,  # Reduced for demo\n",
        "    validation_split=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"✅ Hyperparameter optimization completed!\")\n",
        "print(f\"📊 Optimized {len(optimization_results)} estimators\")\n",
        "\n",
        "# Display optimization results\n",
        "for estimator_type, result in optimization_results.items():\n",
        "    print(f\"\\n{estimator_type}:\")\n",
        "    print(f\"  🎯 Best score: {result.best_value:.4f}\")\n",
        "    print(f\"  ⚙️  Best params: {result.best_params}\")\n",
        "    print(f\"  🔄 Trials: {result.n_trials}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Pretrained Models and Inference\n",
        "\n",
        "Let's explore the pretrained model system for efficient model storage, loading, and inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize pretrained model manager\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create temporary directory for models\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "print(f\"📁 Using temporary directory: {temp_dir}\")\n",
        "\n",
        "# Initialize pretrained model manager\n",
        "model_manager = PretrainedModelManager(temp_dir)\n",
        "\n",
        "# Create training data for pretrained models\n",
        "print(\"Creating training data for pretrained models...\")\n",
        "\n",
        "X_pretrained, y_pretrained, training_info = model_manager.create_training_data(\n",
        "    hurst_values=[0.3, 0.5, 0.7, 0.9],\n",
        "    lengths=[500, 1000, 2000],\n",
        "    n_samples_per_config=10,\n",
        "    generators=['fbm', 'fgn'],\n",
        "    contaminations=['none', 'noise'],\n",
        "    biomedical_scenarios=['eeg', 'ecg']\n",
        ")\n",
        "\n",
        "print(f\"✅ Training data created!\")\n",
        "print(f\"📊 Features shape: {X_pretrained.shape}\")\n",
        "print(f\"🎯 Targets shape: {y_pretrained.shape}\")\n",
        "print(f\"📋 Training info: {len(training_info)} configurations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pretrained model suite\n",
        "print(\"Creating pretrained model suite...\")\n",
        "\n",
        "from neurological_lrd_analysis.ml_baselines.pretrained_models import (\n",
        "    TrainingConfig,\n",
        "    ModelStatus\n",
        ")\n",
        "\n",
        "# Define training configurations\n",
        "training_configs = [\n",
        "    TrainingConfig(\n",
        "        model_type=MLBaselineType.RANDOM_FOREST,\n",
        "        hyperparameters={'n_estimators': 100, 'max_depth': 10, 'random_state': 42},\n",
        "        training_data_config={'n_samples_per_config': 10},\n",
        "        description=\"Random Forest for Hurst estimation\"\n",
        "    ),\n",
        "    TrainingConfig(\n",
        "        model_type=MLBaselineType.SVR,\n",
        "        hyperparameters={'C': 1.0, 'gamma': 'scale', 'epsilon': 0.1},\n",
        "        training_data_config={'n_samples_per_config': 10},\n",
        "        description=\"SVR for Hurst estimation\"\n",
        "    ),\n",
        "    TrainingConfig(\n",
        "        model_type=MLBaselineType.GRADIENT_BOOSTING,\n",
        "        hyperparameters={'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'random_state': 42},\n",
        "        training_data_config={'n_samples_per_config': 10},\n",
        "        description=\"Gradient Boosting for Hurst estimation\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model suite\n",
        "training_results = model_manager.create_model_suite(\n",
        "    training_configs, X_pretrained, y_pretrained, training_info\n",
        ")\n",
        "\n",
        "print(\"✅ Pretrained model suite created!\")\n",
        "print(f\"📊 Trained {len(training_results)} models\")\n",
        "\n",
        "# List available models\n",
        "models = model_manager.list_models(status=ModelStatus.TRAINED)\n",
        "print(f\"📋 Available trained models: {len(models)}\")\n",
        "for model in models:\n",
        "    print(f\"  - {model.model_id}: {model.model_type} ({model.status})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test pretrained model inference\n",
        "print(\"Testing pretrained model inference...\")\n",
        "\n",
        "# Initialize inference system\n",
        "inference_system = PretrainedInference(temp_dir)\n",
        "\n",
        "# Generate test data\n",
        "test_data = fbm_davies_harte(1000, 0.6, seed=42)\n",
        "test_features = feature_extractor.extract_features(test_data)\n",
        "\n",
        "print(f\"📊 Test data: {len(test_data)} samples\")\n",
        "print(f\"🔍 Test features: {len(test_features)} features\")\n",
        "\n",
        "# Single prediction\n",
        "print(\"\\n🎯 Single Prediction:\")\n",
        "single_prediction = inference_system.predict_single(test_features)\n",
        "print(f\"  Predicted Hurst: {single_prediction:.4f}\")\n",
        "\n",
        "# Batch prediction\n",
        "print(\"\\n📦 Batch Prediction:\")\n",
        "batch_features = [feature_extractor.extract_features(fbm_davies_harte(1000, h, seed=42)) for h in [0.3, 0.5, 0.7, 0.9]]\n",
        "batch_predictions = inference_system.predict_batch(batch_features)\n",
        "print(f\"  True Hurst: [0.3, 0.5, 0.7, 0.9]\")\n",
        "print(f\"  Predicted: {[f'{p:.4f}' for p in batch_predictions]}\")\n",
        "\n",
        "# Ensemble prediction\n",
        "print(\"\\n🎭 Ensemble Prediction:\")\n",
        "ensemble_prediction = inference_system.predict_ensemble(test_features)\n",
        "print(f\"  Ensemble Hurst: {ensemble_prediction:.4f}\")\n",
        "\n",
        "# Quick prediction functions\n",
        "print(\"\\n⚡ Quick Predictions:\")\n",
        "quick_pred = quick_predict(test_features, temp_dir)\n",
        "quick_ensemble = quick_ensemble_predict(test_features, temp_dir)\n",
        "print(f\"  Quick prediction: {quick_pred:.4f}\")\n",
        "print(f\"  Quick ensemble: {quick_ensemble:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Pretrained model inference completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comprehensive Benchmarking: Classical vs ML Methods\n",
        "\n",
        "Let's run a comprehensive benchmark comparing classical Hurst estimation methods with our ML baselines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize comprehensive benchmark\n",
        "print(\"Initializing comprehensive benchmark...\")\n",
        "\n",
        "benchmark = ClassicalMLBenchmark(\n",
        "    pretrained_models_dir=temp_dir,\n",
        "    classical_estimators=[\n",
        "        EstimatorType.DFA,\n",
        "        EstimatorType.RS_ANALYSIS,\n",
        "        EstimatorType.HIGUCHI,\n",
        "        EstimatorType.GENERALIZED_HURST\n",
        "    ],\n",
        "    ml_estimators=[\n",
        "        MLBaselineType.RANDOM_FOREST,\n",
        "        MLBaselineType.SVR,\n",
        "        MLBaselineType.GRADIENT_BOOSTING\n",
        "    ],\n",
        "    results_dir=\"./benchmark_results\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"✅ Benchmark initialized!\")\n",
        "print(f\"🔬 Classical methods: {len(benchmark.classical_estimators)}\")\n",
        "print(f\"🤖 ML methods: {len(benchmark.ml_estimators)}\")\n",
        "\n",
        "# Generate test scenarios\n",
        "print(\"\\nGenerating test scenarios...\")\n",
        "test_scenarios = []\n",
        "\n",
        "# Generate diverse test scenarios\n",
        "hurst_test_values = [0.3, 0.5, 0.7, 0.9]\n",
        "for hurst in hurst_test_values:\n",
        "    # FBM data\n",
        "    fbm_data = fbm_davies_harte(1000, hurst, seed=42)\n",
        "    from neurological_lrd_analysis.benchmark_core.generation import TimeSeriesSample\n",
        "    sample = TimeSeriesSample(\n",
        "        data=fbm_data,\n",
        "        true_hurst=hurst,\n",
        "        length=len(fbm_data),\n",
        "        contamination='none',\n",
        "        seed=42\n",
        "    )\n",
        "    test_scenarios.append(sample)\n",
        "    \n",
        "    # FGN data\n",
        "    fgn_data = generate_fgn(1000, hurst, seed=42)\n",
        "    sample = TimeSeriesSample(\n",
        "        data=fgn_data,\n",
        "        true_hurst=hurst,\n",
        "        length=len(fgn_data),\n",
        "        contamination='none',\n",
        "        seed=42\n",
        "    )\n",
        "    test_scenarios.append(sample)\n",
        "\n",
        "print(f\"📊 Generated {len(test_scenarios)} test scenarios\")\n",
        "print(f\"🎯 Hurst range: {min([s.true_hurst for s in test_scenarios]):.1f} to {max([s.true_hurst for s in test_scenarios]):.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive benchmark\n",
        "print(\"Running comprehensive benchmark...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the benchmark\n",
        "benchmark_results = benchmark.run_comprehensive_benchmark(\n",
        "    test_scenarios=test_scenarios,\n",
        "    save_results=True\n",
        ")\n",
        "\n",
        "benchmark_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n✅ Benchmark completed in {benchmark_time:.2f} seconds!\")\n",
        "print(f\"📊 Results saved to: {benchmark.results_dir}\")\n",
        "\n",
        "# Display benchmark summary\n",
        "if 'summaries' in benchmark_results and benchmark_results['summaries']:\n",
        "    print(\"\\n📈 Benchmark Summary:\")\n",
        "    benchmark.print_benchmark_summary(benchmark_results['summaries'])\n",
        "else:\n",
        "    print(\"\\n⚠️ No benchmark summaries available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize benchmark results\n",
        "if 'summaries' in benchmark_results and benchmark_results['summaries']:\n",
        "    print(\"Creating benchmark visualizations...\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    benchmark.create_visualizations(\n",
        "        benchmark_results, \n",
        "        output_path=\"./benchmark_plots.png\"\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Visualizations created!\")\n",
        "    print(\"📊 Check './benchmark_plots.png' for detailed plots\")\n",
        "    \n",
        "    # Display results table\n",
        "    summaries = benchmark_results['summaries']\n",
        "    \n",
        "    print(\"\\n📊 Detailed Results:\")\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"{'Method':<25} {'Type':<10} {'MAE':<10} {'RMSE':<10} {'Corr':<10} {'Time(ms)':<12} {'Success':<10}\")\n",
        "    print(\"-\" * 100)\n",
        "    \n",
        "    for method_name, summary in summaries.items():\n",
        "        print(f\"{method_name:<25} {summary.method_type:<10} {summary.mean_absolute_error:<10.4f} \"\n",
        "              f\"{summary.root_mean_squared_error:<10.4f} {summary.correlation:<10.4f} \"\n",
        "              f\"{summary.mean_computation_time*1000:<12.1f} {summary.success_rate:<10.2f}\")\n",
        "    \n",
        "    # Find best performers\n",
        "    if summaries:\n",
        "        best_mae = min(summaries.items(), key=lambda x: x[1].mean_absolute_error)\n",
        "        best_speed = min(summaries.items(), key=lambda x: x[1].mean_computation_time)\n",
        "        \n",
        "        print(f\"\\n🏆 Best Performance:\")\n",
        "        print(f\"  🎯 Lowest MAE: {best_mae[0]} ({best_mae[1].mean_absolute_error:.4f})\")\n",
        "        print(f\"  ⚡ Fastest: {best_speed[0]} ({best_speed[1].mean_computation_time*1000:.1f}ms)\")\n",
        "        \n",
        "        # Compare classical vs ML\n",
        "        classical_methods = {k: v for k, v in summaries.items() if v.method_type == 'classical'}\n",
        "        ml_methods = {k: v for k, v in summaries.items() if v.method_type == 'ml'}\n",
        "        \n",
        "        if classical_methods and ml_methods:\n",
        "            best_classical = min(classical_methods.items(), key=lambda x: x[1].mean_absolute_error)\n",
        "            best_ml = min(ml_methods.items(), key=lambda x: x[1].mean_absolute_error)\n",
        "            \n",
        "            print(f\"\\n🔬 Classical vs ML Comparison:\")\n",
        "            print(f\"  🎯 Best Classical: {best_classical[0]} ({best_classical[1].mean_absolute_error:.4f})\")\n",
        "            print(f\"  🤖 Best ML: {best_ml[0]} ({best_ml[1].mean_absolute_error:.4f})\")\n",
        "            \n",
        "            if best_ml[1].mean_absolute_error < best_classical[1].mean_absolute_error:\n",
        "                improvement = ((best_classical[1].mean_absolute_error - best_ml[1].mean_absolute_error) / \n",
        "                              best_classical[1].mean_absolute_error) * 100\n",
        "                print(f\"  🚀 ML improvement: {improvement:.1f}%\")\n",
        "            else:\n",
        "                print(f\"  📊 Classical methods perform better in this benchmark\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ No benchmark results available for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Conclusions\n",
        "\n",
        "This tutorial demonstrated the comprehensive ML capabilities of the Neurological LRD Analysis library:\n",
        "\n",
        "### 🎯 **Key Features Explored**\n",
        "\n",
        "1. **Feature Extraction**: 74+ features across statistical, spectral, wavelet, and fractal categories\n",
        "2. **ML Baselines**: Random Forest, SVR, and Gradient Boosting for Hurst estimation\n",
        "3. **Hyperparameter Optimization**: Optuna integration for automatic parameter tuning\n",
        "4. **Pretrained Models**: Efficient model storage, loading, and inference systems\n",
        "5. **Comprehensive Benchmarking**: Direct comparison between classical and ML methods\n",
        "\n",
        "### 📊 **Performance Insights**\n",
        "\n",
        "- **ML methods** often provide more accurate Hurst estimates than classical methods\n",
        "- **Feature extraction** is crucial for ML-based estimation\n",
        "- **Hyperparameter optimization** can significantly improve model performance\n",
        "- **Pretrained models** enable fast inference for production systems\n",
        "- **Ensemble methods** can combine multiple models for robust predictions\n",
        "\n",
        "### 🚀 **Next Steps**\n",
        "\n",
        "1. **Experiment with different feature sets** for your specific applications\n",
        "2. **Train custom models** on your domain-specific data\n",
        "3. **Use pretrained models** for rapid prototyping and production deployment\n",
        "4. **Run comprehensive benchmarks** to validate method performance on your data\n",
        "5. **Explore ensemble methods** for improved accuracy and robustness\n",
        "\n",
        "### 📚 **Additional Resources**\n",
        "\n",
        "- **Documentation**: Check the library documentation for detailed API references\n",
        "- **Examples**: Explore other notebooks for specific use cases\n",
        "- **Benchmarks**: Run your own benchmarks with custom datasets\n",
        "- **Contributions**: Contribute to the library development and improvement\n",
        "\n",
        "---\n",
        "\n",
        "**Happy analyzing! 🧠📈**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
